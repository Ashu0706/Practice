{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------+------+----------+\n",
      "|emp_id|emp_name|   dept|salary| join_date|\n",
      "+------+--------+-------+------+----------+\n",
      "|     1|    Amit|     IT| 70000|2022-01-10|\n",
      "|     2|   Rahul|     IT| 60000|2021-05-12|\n",
      "|     3|   Priya|     IT| 60000|2023-03-15|\n",
      "|     4|    Neha|     HR| 50000|2020-07-19|\n",
      "|     5|   Karan|     HR| 80000|2019-11-01|\n",
      "|     6|   Rohit|Finance| 90000|2018-04-23|\n",
      "|     7|  Simran|Finance| 40000|2022-08-30|\n",
      "|     8|   Arjun|     IT| 75000|2020-09-14|\n",
      "+------+--------+-------+------+----------+\n",
      "\n",
      "root\n",
      " |-- emp_id: integer (nullable = true)\n",
      " |-- emp_name: string (nullable = true)\n",
      " |-- dept: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- join_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType\n",
    "from datetime import date\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Practice\").getOrCreate()\n",
    "\n",
    "# Define Schema\n",
    "schema = StructType([\n",
    "    StructField(\"emp_id\", IntegerType(), True),\n",
    "    StructField(\"emp_name\", StringType(), True),\n",
    "    StructField(\"dept\", StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True),\n",
    "    StructField(\"join_date\", DateType(), True)\n",
    "])\n",
    "\n",
    "# Sample Data\n",
    "data = [\n",
    "    (1, \"Amit\", \"IT\", 70000, date(2022,1,10)),\n",
    "    (2, \"Rahul\", \"IT\", 60000, date(2021,5,12)),\n",
    "    (3, \"Priya\", \"IT\", 60000, date(2023,3,15)),\n",
    "    (4, \"Neha\", \"HR\", 50000, date(2020,7,19)),\n",
    "    (5, \"Karan\", \"HR\", 80000, date(2019,11,1)),\n",
    "    (6, \"Rohit\", \"Finance\", 90000, date(2018,4,23)),\n",
    "    (7, \"Simran\", \"Finance\", 40000, date(2022,8,30)),\n",
    "    (8, \"Arjun\", \"IT\", 75000, date(2020,9,14))\n",
    "]\n",
    "\n",
    "employee = spark.createDataFrame(data, schema)\n",
    "\n",
    "employee.show()\n",
    "employee.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+--------+------+\n",
      "|order_id|emp_id| product|amount|\n",
      "+--------+------+--------+------+\n",
      "|     101|     1|  Laptop| 50000|\n",
      "|     102|     2|  Mobile| 20000|\n",
      "|     103|     1|Keyboard|  2000|\n",
      "|     104|     3|  Laptop| 50000|\n",
      "|     105|     2|   Mouse|  1000|\n",
      "|     106|     3| Monitor| 10000|\n",
      "+--------+------+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "order_schema = StructType([\n",
    "    StructField(\"order_id\", IntegerType(), True),\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"product\", StringType(), True),\n",
    "    StructField(\"amount\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "order_data = [\n",
    "    (101, 1, \"Laptop\", 50000),\n",
    "    (102, 2, \"Mobile\", 20000),\n",
    "    (103, 1, \"Keyboard\", 2000),\n",
    "    (104, 3, \"Laptop\", 50000),\n",
    "    (105, 2, \"Mouse\", 1000),\n",
    "    (106, 3, \"Monitor\", 10000)\n",
    "]\n",
    "\n",
    "orders = spark.createDataFrame(order_data, [\"order_id\", \"emp_id\", \"product\", \"amount\"])\n",
    "\n",
    "orders.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------+------+----------+\n",
      "|emp_id|emp_name|   dept|salary| join_date|\n",
      "+------+--------+-------+------+----------+\n",
      "|     1|    Amit|     IT| 70000|2022-01-10|\n",
      "|     2|   Rahul|     IT| 60000|2021-05-12|\n",
      "|     3|   Priya|     IT| 60000|2023-03-15|\n",
      "|     4|    Neha|     HR| 50000|2020-07-19|\n",
      "|     5|   Karan|     HR| 80000|2019-11-01|\n",
      "|     6|   Rohit|Finance| 90000|2018-04-23|\n",
      "|     7|  Simran|Finance| 40000|2022-08-30|\n",
      "|     8|   Arjun|     IT| 75000|2020-09-14|\n",
      "+------+--------+-------+------+----------+\n",
      "\n",
      "+--------+------+--------+------+\n",
      "|order_id|emp_id| product|amount|\n",
      "+--------+------+--------+------+\n",
      "|     101|     1|  Laptop| 50000|\n",
      "|     102|     2|  Mobile| 20000|\n",
      "|     103|     1|Keyboard|  2000|\n",
      "|     104|     3|  Laptop| 50000|\n",
      "|     105|     2|   Mouse|  1000|\n",
      "|     106|     3| Monitor| 10000|\n",
      "+--------+------+--------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "employee.show(),orders.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "employee.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------+------+----------+---------+--------+\n",
      "|emp_id|emp_name|   dept|salary| join_date|range_sum|rows_sum|\n",
      "+------+--------+-------+------+----------+---------+--------+\n",
      "|     4|    Neha|     HR| 50000|2020-07-19|    50000|   50000|\n",
      "|     5|   Karan|     HR| 80000|2019-11-01|   130000|  130000|\n",
      "|     7|  Simran|Finance| 40000|2022-08-30|    40000|   40000|\n",
      "|     6|   Rohit|Finance| 90000|2018-04-23|   130000|  130000|\n",
      "|     2|   Rahul|     IT| 60000|2021-05-12|   120000|   60000|\n",
      "|     3|   Priya|     IT| 60000|2023-03-15|   120000|  120000|\n",
      "|     1|    Amit|     IT| 70000|2022-01-10|   190000|  190000|\n",
      "|     8|   Arjun|     IT| 75000|2020-09-14|   265000|  265000|\n",
      "+------+--------+-------+------+----------+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "window_range = Window.partitionBy(\"dept\").orderBy(\"salary\")\n",
    "\n",
    "window_rows = Window.partitionBy(\"dept\") \\\n",
    "                    .orderBy(\"salary\") \\\n",
    "                    .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "employee.withColumn(\"range_sum\", sum(\"salary\").over(window_range)) \\\n",
    "        .withColumn(\"rows_sum\", sum(\"salary\").over(window_rows)) \\\n",
    "        .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+------+\n",
      "|emp_name|dept|salary|\n",
      "+--------+----+------+\n",
      "|   Rahul|  IT| 60000|\n",
      "+--------+----+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col,dense_rank,row_number\n",
    "\n",
    "window = Window.partitionBy('dept').orderBy(col('salary').desc())\n",
    "highest_sal_per_dept = employee.withColumn('rnk',row_number().over(window)).filter(col('rnk') == 3).select('emp_name','dept','salary')\n",
    "highest_sal_per_dept.show()\n",
    "\n",
    "highest_sal_per_dept.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+\n",
      "|   dept|third_salary|\n",
      "+-------+------------+\n",
      "|     HR|       50000|\n",
      "|Finance|       40000|\n",
      "|     IT|       70000|\n",
      "+-------+------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'200'"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import max, when\n",
    "\n",
    "result = employee \\\n",
    "    .withColumn(\"rn\", row_number().over(window)) \\\n",
    "    .groupBy(\"dept\") \\\n",
    "    .agg(\n",
    "        max(when(col(\"rn\") == 2, col(\"salary\"))).alias(\"third_salary\")\n",
    "    )\n",
    "\n",
    "result.show()\n",
    "\n",
    "result.rdd.getNumPartitions()\n",
    "spark.conf.get(\"spark.sql.shuffle.partitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------+\n",
      "|dept|total_salary|\n",
      "+----+------------+\n",
      "|  IT|      265000|\n",
      "+----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum, col\n",
    "\n",
    "result = employee \\\n",
    "    .groupBy(\"dept\") \\\n",
    "    .agg(sum(\"salary\").alias(\"total_salary\")) \\\n",
    "    .orderBy(col(\"total_salary\").desc()) \\\n",
    "    .limit(1)\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+------+--------+\n",
      "|emp_name|   dept|salary|dept_avg|\n",
      "+--------+-------+------+--------+\n",
      "|   Karan|     HR| 80000| 65000.0|\n",
      "|   Rohit|Finance| 90000| 65000.0|\n",
      "|    Amit|     IT| 70000| 66250.0|\n",
      "|   Arjun|     IT| 75000| 66250.0|\n",
      "+--------+-------+------+--------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, avg\n",
    "\n",
    "window = Window.partitionBy(\"dept\")\n",
    "\n",
    "result = employee \\\n",
    "    .withColumn(\"dept_avg\", avg(\"salary\").over(window)) \\\n",
    "    .filter(col(\"salary\") > col(\"dept_avg\")) \\\n",
    "    .select(\"emp_name\", \"dept\", \"salary\", \"dept_avg\")\n",
    "\n",
    "result.show()\n",
    "result.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"False\")\n",
    "#spark.conf.set(\"spark.sql.shuffle.partitions\", \"200\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial partitions: 8\n",
      "200\n",
      "After groupBy partitions: 200\n",
      "== Parsed Logical Plan ==\n",
      "Range (0, 100000, step=1, splits=Some(8))\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "id: bigint\n",
      "Range (0, 100000, step=1, splits=Some(8))\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Range (0, 100000, step=1, splits=Some(8))\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Range (0, 100000, step=1, splits=8)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "df = spark.range(100000)\n",
    "\n",
    "print(\"Initial partitions:\", df.rdd.getNumPartitions())\n",
    "\n",
    "df2 = df.groupBy((df.id % 5)).count()\n",
    "print(df2.rdd.getNumPartitions())\n",
    "\n",
    "\n",
    "print(\"After groupBy partitions:\", df2.rdd.getNumPartitions())\n",
    "df.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'False'"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.sql.adaptive.enabled\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
