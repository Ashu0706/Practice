{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/21 00:12:22 WARN Utils: Your hostname, ASHUTOSHs-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 192.168.1.100 instead (on interface en0)\n",
      "26/02/21 00:12:22 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/02/21 00:12:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------+------+----------+\n",
      "|emp_id|emp_name|   dept|salary| join_date|\n",
      "+------+--------+-------+------+----------+\n",
      "|     1|    Amit|     IT| 70000|2022-01-10|\n",
      "|     2|   Rahul|     IT| 60000|2021-05-12|\n",
      "|     3|   Priya|     IT| 60000|2023-03-15|\n",
      "|     4|    Neha|     HR| 50000|2020-07-19|\n",
      "|     5|   Karan|     HR| 80000|2019-11-01|\n",
      "|     6|   Rohit|Finance| 90000|2018-04-23|\n",
      "|     7|  Simran|Finance| 40000|2022-08-30|\n",
      "|     8|   Arjun|     IT| 75000|2020-09-14|\n",
      "+------+--------+-------+------+----------+\n",
      "\n",
      "root\n",
      " |-- emp_id: integer (nullable = true)\n",
      " |-- emp_name: string (nullable = true)\n",
      " |-- dept: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- join_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType\n",
    "from datetime import date\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Practice\").getOrCreate()\n",
    "\n",
    "# Define Schema\n",
    "schema = StructType([\n",
    "    StructField(\"emp_id\", IntegerType(), True),\n",
    "    StructField(\"emp_name\", StringType(), True),\n",
    "    StructField(\"dept\", StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True),\n",
    "    StructField(\"join_date\", DateType(), True)\n",
    "])\n",
    "\n",
    "# Sample Data\n",
    "data = [\n",
    "    (1, \"Amit\", \"IT\", 70000, date(2022,1,10)),\n",
    "    (2, \"Rahul\", \"IT\", 60000, date(2021,5,12)),\n",
    "    (3, \"Priya\", \"IT\", 60000, date(2023,3,15)),\n",
    "    (4, \"Neha\", \"HR\", 50000, date(2020,7,19)),\n",
    "    (5, \"Karan\", \"HR\", 80000, date(2019,11,1)),\n",
    "    (6, \"Rohit\", \"Finance\", 90000, date(2018,4,23)),\n",
    "    (7, \"Simran\", \"Finance\", 40000, date(2022,8,30)),\n",
    "    (8, \"Arjun\", \"IT\", 75000, date(2020,9,14))\n",
    "]\n",
    "\n",
    "employee = spark.createDataFrame(data, schema)\n",
    "\n",
    "employee.show()\n",
    "employee.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+--------+------+\n",
      "|order_id|emp_id| product|amount|\n",
      "+--------+------+--------+------+\n",
      "|     101|     1|  Laptop| 50000|\n",
      "|     102|     2|  Mobile| 20000|\n",
      "|     103|     1|Keyboard|  2000|\n",
      "|     104|     3|  Laptop| 50000|\n",
      "|     105|     2|   Mouse|  1000|\n",
      "|     106|     3| Monitor| 10000|\n",
      "+--------+------+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "order_schema = StructType([\n",
    "    StructField(\"order_id\", IntegerType(), True),\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"product\", StringType(), True),\n",
    "    StructField(\"amount\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "order_data = [\n",
    "    (101, 1, \"Laptop\", 50000),\n",
    "    (102, 2, \"Mobile\", 20000),\n",
    "    (103, 1, \"Keyboard\", 2000),\n",
    "    (104, 3, \"Laptop\", 50000),\n",
    "    (105, 2, \"Mouse\", 1000),\n",
    "    (106, 3, \"Monitor\", 10000)\n",
    "]\n",
    "\n",
    "orders = spark.createDataFrame(order_data, [\"order_id\", \"emp_id\", \"product\", \"amount\"])\n",
    "\n",
    "orders.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------+------+----------+\n",
      "|emp_id|emp_name|   dept|salary| join_date|\n",
      "+------+--------+-------+------+----------+\n",
      "|     1|    Amit|     IT| 70000|2022-01-10|\n",
      "|     2|   Rahul|     IT| 60000|2021-05-12|\n",
      "|     3|   Priya|     IT| 60000|2023-03-15|\n",
      "|     4|    Neha|     HR| 50000|2020-07-19|\n",
      "|     5|   Karan|     HR| 80000|2019-11-01|\n",
      "|     6|   Rohit|Finance| 90000|2018-04-23|\n",
      "|     7|  Simran|Finance| 40000|2022-08-30|\n",
      "|     8|   Arjun|     IT| 75000|2020-09-14|\n",
      "+------+--------+-------+------+----------+\n",
      "\n",
      "+--------+------+--------+------+\n",
      "|order_id|emp_id| product|amount|\n",
      "+--------+------+--------+------+\n",
      "|     101|     1|  Laptop| 50000|\n",
      "|     102|     2|  Mobile| 20000|\n",
      "|     103|     1|Keyboard|  2000|\n",
      "|     104|     3|  Laptop| 50000|\n",
      "|     105|     2|   Mouse|  1000|\n",
      "|     106|     3| Monitor| 10000|\n",
      "+--------+------+--------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "employee.show(),orders.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------+------+----------+---------+--------+\n",
      "|emp_id|emp_name|   dept|salary| join_date|range_sum|rows_sum|\n",
      "+------+--------+-------+------+----------+---------+--------+\n",
      "|     7|  Simran|Finance| 40000|2022-08-30|    40000|   40000|\n",
      "|     6|   Rohit|Finance| 90000|2018-04-23|   130000|  130000|\n",
      "|     4|    Neha|     HR| 50000|2020-07-19|    50000|   50000|\n",
      "|     5|   Karan|     HR| 80000|2019-11-01|   130000|  130000|\n",
      "|     2|   Rahul|     IT| 60000|2021-05-12|   120000|   60000|\n",
      "|     3|   Priya|     IT| 60000|2023-03-15|   120000|  120000|\n",
      "|     1|    Amit|     IT| 70000|2022-01-10|   190000|  190000|\n",
      "|     8|   Arjun|     IT| 75000|2020-09-14|   265000|  265000|\n",
      "+------+--------+-------+------+----------+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "window_range = Window.partitionBy(\"dept\").orderBy(\"salary\")\n",
    "\n",
    "window_rows = Window.partitionBy(\"dept\") \\\n",
    "                    .orderBy(\"salary\") \\\n",
    "                    .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "employee.withColumn(\"range_sum\", sum(\"salary\").over(window_range)) \\\n",
    "        .withColumn(\"rows_sum\", sum(\"salary\").over(window_rows)) \\\n",
    "        .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+------+\n",
      "|emp_name|dept|salary|\n",
      "+--------+----+------+\n",
      "|   Rahul|  IT| 60000|\n",
      "+--------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col,dense_rank,row_number\n",
    "\n",
    "window = Window.partitionBy('dept').orderBy(col('salary').desc())\n",
    "highest_sal_per_dept = employee.withColumn('rnk',row_number().over(window)).filter(col('rnk') == 3).select('emp_name','dept','salary')\n",
    "highest_sal_per_dept.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+\n",
      "|   dept|third_salary|\n",
      "+-------+------------+\n",
      "|Finance|       40000|\n",
      "|     HR|       50000|\n",
      "|     IT|       70000|\n",
      "+-------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import max, when\n",
    "\n",
    "result = employee \\\n",
    "    .withColumn(\"rn\", row_number().over(window)) \\\n",
    "    .groupBy(\"dept\") \\\n",
    "    .agg(\n",
    "        max(when(col(\"rn\") == 2, col(\"salary\"))).alias(\"third_salary\")\n",
    "    )\n",
    "\n",
    "result.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
